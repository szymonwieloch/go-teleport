// Repository and reader of stdout and stderr generated by the given job
package jobs

import (
	"bufio"
	"io"
	"log"
	"sync"
	"time"
)

// Represents a single obtained log line with metadata
type LogEntry struct {
	Line      string
	Timestamp time.Time
	Stdout    bool
}

// Repository of logs
type logs struct {
	sync.Mutex
	cond         *sync.Cond
	readingCoros int
	logs         []LogEntry
	jobID        JobID
}

// Bacground job that reads lines from n output process stream
func (logs *logs) read(pipe io.ReadCloser, stdout bool) {
	reader := bufio.NewReader(pipe)
	name := "stderr"
	if stdout {
		name = "stdout"
	}
	for {
		line, err := reader.ReadString('\n')
		if err != nil {
			log.Println("pipe", name, "of job", logs.jobID, "got closed")
			break
		}
		entry := LogEntry{Line: line, Timestamp: time.Now(), Stdout: stdout}
		logs.append(entry)
	}
	logs.Lock()
	defer logs.Unlock()
	logs.readingCoros -= 1
	logs.cond.Broadcast()
}

// Appends a line to the repository of logs
// Thread safe
func (logs *logs) append(entry LogEntry) {
	logs.Lock()
	defer logs.Unlock()
	logs.logs = append(logs.logs, entry)
	logs.cond.Broadcast()
}

// Gets a slice with logs or waits until they are generated.
// Returning 0 length indicates that there are no more logs to return
func (logs *logs) get(start, maxCount int) []LogEntry {
	logs.Lock()
	defer logs.Unlock()
	for start >= len(logs.logs) && logs.readingCoros > 0 {
		logs.cond.Wait()
	}
	return logs.logs[start:min(start+maxCount, len(logs.logs))]
}

// Returns the number of kept logs
func (logs *logs) size() int {
	logs.Lock()
	defer logs.Unlock()
	return len(logs.logs)
}

// Creates a new instance of "logs"
func newLogs(stdout, stderr io.ReadCloser, jobID JobID) *logs {
	result := &logs{readingCoros: 2, jobID: jobID}
	result.cond = sync.NewCond(result)
	go result.read(stdout, true)
	go result.read(stderr, false)
	return result
}
